[[{"text":"Q. In what sense does RL blur the training and testing phases of traditional ML?\nA. While training an RL agent, you're evaluating it and improving its ability simultaneously.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/div[1]/p[2]","startOffset":245,"endContainer":"/div[1]/div[1]/p[2]","endOffset":330},{"type":"TextPositionSelector","start":2948,"end":3033},{"type":"TextQuoteSelector","exact":"while training RL, you're both evaluating it and improving its ability simultaneously","prefix":" the task you’ve set for it. So ","suffix":".Aside: this isn't the case in a"}]}]},{"text":"Q. What does TD Learning an abbreviation for?\nA. Temporal-Difference Learning","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[14]","startOffset":0,"endContainer":"/div[1]/p[14]","endOffset":33},{"type":"TextPositionSelector","start":3254,"end":3287},{"type":"TextQuoteSelector","exact":"Temporal-Difference (TD) Learning","prefix":"\n2. Temporal-Difference Learning","suffix":" is the first Reinforcement Lear"}]}]},{"text":"Q. What does TD Learning estimate?\nA. The value function for a given policy.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[17]","startOffset":0,"endContainer":"/div[1]/p[17]","endOffset":148},{"type":"TextPositionSelector","start":3701,"end":3849},{"type":"TextQuoteSelector","exact":"TD Learning estimates the value function for a given policy, @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')π\\piπ﻿.","prefix":"n) from each state than another.","suffix":"\nTD Learning is an ‘online’ lear"}]}]},{"text":"Q. How can you estimate a policy's value function without knowing the state transition function?\nA. e.g. by taking actions in an environment and directly observing the resulting state","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[15]","startOffset":0,"endContainer":"/div[1]/p[15]","endOffset":139},{"type":"TextPositionSelector","start":3352,"end":3491},{"type":"TextQuoteSelector","exact":"It learns a value function estimate directly from interacting with the environment - without needing to know the state transition function.","prefix":"ing algorithm we’re introducing.","suffix":"In Tutorial 2, we saw that the v"}]}]},{"text":"Q. Why might it be hard to determine an RL environment's state transition function?\nA. e.g. for a self-driving car, you lack a perfect model of how other cars will move","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[7]","startOffset":132,"endContainer":"/div[1]/p[7]","endOffset":231},{"type":"TextPositionSelector","start":1303,"end":1402},{"type":"TextQuoteSelector","exact":"we don't have a perfect model of how other cars and pedestrians will move or react to our movements","prefix":"e future state. This is because ","suffix":".How the state evolves in respon"}]}]},{"text":"Q. What does it mean that TD Learning is an \"online\" learning algorithm?\nA. It learns after every step, not by retrospectively analyzing bulk data.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[19]","startOffset":0,"endContainer":"/div[1]/p[19]","endOffset":100},{"type":"TextPositionSelector","start":3850,"end":3950},{"type":"TextQuoteSelector","exact":"TD Learning is an ‘online’ learning algorithm. This means it learns after every step of experience. ","prefix":"X/0.13.2/katex.min.css')π\\piπ﻿.\n","suffix":"This is in contrast to algorithm"}]}]},{"text":"Q. What three things happen in each step of the TD Learning algorithm? \nA. Choose an action using our policy; observe the reward and successor state from the environment; update the value function estimate using the TD Update Equation.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/ol[3]/li[1]/strong[1]","startOffset":0,"endContainer":"/div[1]/ol[5]/li[1]","endOffset":90},{"type":"TextPositionSelector","start":4262,"end":4622},{"type":"TextQuoteSelector","exact":"Get the action using our estimate of the value of state @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')sss﻿Observe the reward and next state from the environment (decided by the reward function and state transition function respectively)Update the value function estimate for that state using the TD Update Equation (see below)","prefix":"statesInitialise the environment","suffix":"Go back to step 3. until the epi"}]}]},{"text":"Q. Give the TD Update Equation.\nA. $\\hat{v}(s_t) \\leftarrow (1 - \\alpha)\\hat{v}(s_t) + \\alpha [r_{t+1} + \\gamma \\hat{v}(s_{t+1})]$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[24]","startOffset":70,"endContainer":"/div[1]/p[24]","endOffset":130},{"type":"TextPositionSelector","start":5028,"end":5088},{"type":"TextQuoteSelector","exact":"below is the way we think is most intuitively understandable","prefix":"he TD Update Equation is shown, ","suffix":". There’s a lot of notation here"}]}]},{"text":"Q. What does the hat over the $v$ in $\\hat v(s_t)$ denote for a value function?\nA. That it is an estimate, not the true value.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/ul[1]/li[1]/details[1]/p[2]","startOffset":0,"endContainer":"/div[1]/ul[1]/li[1]/details[1]/p[2]/strong[1]","endOffset":8},{"type":"TextPositionSelector","start":6087,"end":6225},{"type":"TextQuoteSelector","exact":"The little hat over the @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')vvv﻿ simply means it’s an estimate","prefix":"0.13.2/katex.min.css')sts_tst​﻿.","suffix":", rather than the true value fun"}]}]},{"text":"Q. What does $\\alpha$ represent in the TD Update Equation?\nA. The step size parameter, a number where $0 < \\alpha < 1$. Large values give more weight to the most recent estimates.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/ul[2]/li[1]/details[1]/summary[1]/strong[1]","startOffset":0,"endContainer":"/div[1]/ul[2]/li[1]/details[1]/summary[1]/span[1]/span[3]","endOffset":1},{"type":"TextPositionSelector","start":6289,"end":6398},{"type":"TextQuoteSelector","exact":"step-size parameter @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')α\\alphaα﻿","prefix":"ction of the current policy.The ","suffix":"It is a number where @import url"}]}]},{"text":"Q. What's a typical value for $\\alpha$ in TD Learning?\nA. $0.2$.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/ul[2]/li[1]/details[1]/p[2]","startOffset":0,"endContainer":"/div[1]/ul[2]/li[1]/details[1]/p[2]","endOffset":158},{"type":"TextPositionSelector","start":6523,"end":6681},{"type":"TextQuoteSelector","exact":"A typical example which can be used in the exercises we’ll face is @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')0.20.20.2﻿.","prefix":"in.css')0<α<10 < \\alpha<10<α<1﻿.","suffix":"This affects how quickly the val"}]}]},{"text":"Q. How should we interpret the two terms of the TD Update Equation, $\\hat{v}(s_t) \\leftarrow (1 - \\alpha)\\hat{v}(s_t) + \\alpha [r_{t+1} + \\gamma \\hat{v}(s_{t+1})]$?\nA. We take the weighted sum of 1) the previous estimate and 2) the value we just experienced.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[29]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[29]","endOffset":38},{"type":"TextPositionSelector","start":6916,"end":6954},{"type":"TextQuoteSelector","exact":"Interpretation: this has 2 main terms:","prefix":"e and learning can be unstable!\n","suffix":"@import url('https://cdnjs.cloud"}]}]},{"text":"Q. In what sense does TD Learning do \"bootstrapping\"?\nA. It learns by updating estimates of value ($\\hat v(s_t)$) using estimates of other states' values ($\\hat v(s_{t+1})$).","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[34]","startOffset":319,"endContainer":"/div[1]/p[34]","endOffset":352},{"type":"TextPositionSelector","start":10477,"end":10510},{"type":"TextQuoteSelector","exact":"This is known as 'bootstrapping'.","prefix":"at{v}(s_{t+1})v^(st+1​)﻿ term). ","suffix":"\n\n\n3. The Env classBelow is the "}]}]},{"text":"Q. Why is it necessary to balance exploration and exploitation when training RL agents?\nA. An agent which only exploits its best-guess policy may never explore a chain of actions which would produce a better return; an agent which only explores may spend much of its time in suboptimal states.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[64]","startOffset":217,"endContainer":"/div[1]/p[64]/strong[1]","endOffset":14},{"type":"TextPositionSelector","start":18789,"end":18857},{"type":"TextQuoteSelector","exact":"An agent which only exploits or only explores tends to perform badly","prefix":"u like. The same is true in RL. ","suffix":". Hence some amount of both of t"}]}]},{"text":"Q. What's meant by \"exploitation\" in RL, in the context of the explore-exploit trade-off?\nA. Taking the action which historically yielded the highest highest return (i.e. the greedy action).","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[60]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[60]/strong[1]","endOffset":8},{"type":"TextPositionSelector","start":17893,"end":17901},{"type":"TextQuoteSelector","exact":"exploits","prefix":"past have yielded rewards. This ","suffix":" its existing knowledge of the e"}]}]},{"text":"Q. What's meant by \"exploration\" in RL, in the context of the explore-exploit trade-off?\nA. Trying under-explored actions which might yield greater returns.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[61]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[61]/strong[1]","endOffset":9},{"type":"TextPositionSelector","start":18152,"end":18161},{"type":"TextQuoteSelector","exact":"exploring","prefix":" with unknown outcomes. This is ","suffix":".Successful actions discovered t"}]}]},{"text":"Q. What does $q_\\pi$ denote in a Markov decision process?\nA. The action-value function.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[68]","startOffset":470,"endContainer":"/div[1]/blockquote[1]","endOffset":0},{"type":"TextPositionSelector","start":19501,"end":19653},{"type":"TextQuoteSelector","exact":"The action-value function is usually denoted @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')qπ(s,a)q_\\pi(s,a)qπ​(s,a)﻿.","prefix":"atex.min.css')(s,a)(s,a)(s,a)﻿. ","suffix":"The action-value @import url('ht"}]}]},{"text":"Q. Give the equation of an action-value function which uses a 1-step lookahead.\nA. $\\hat{q}(s,a) = r(s, a, s') + \\gamma \\times \\hat{v}(s')$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[72]","startOffset":0,"endContainer":"/div[1]/p[72]","endOffset":63},{"type":"TextPositionSelector","start":21045,"end":21108},{"type":"TextQuoteSelector","exact":"A 1-step lookahead simply uses Bellman’s Equation like follows:","prefix":"aTeX/0.13.2/katex.min.css')aaa﻿.","suffix":"@import url('https://cdnjs.cloud"}]}]},{"text":"Q. Give a definition in words of the action-value function, $q_\\pi(s,a)$.\nA. The value associated with taking action $a$ from state $s$ and then following policy $\\pi$.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/blockquote[1]","startOffset":249,"endContainer":"/div[1]/blockquote[1]/span[6]/span[3]","endOffset":1},{"type":"TextPositionSelector","start":19902,"end":20232},{"type":"TextQuoteSelector","exact":" value associated with taking action @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')aaa﻿ from state @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')sss﻿ and then following policy @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')π\\piπ﻿","prefix":"min.css')(s,a)(s,a)(s,a)﻿ is the","suffix":".\nFor now, we’ll calculate actio"}]}]},{"text":"Q. What's the greedy action in RL?\nA. The action with the highest estimated action-value for that state.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[75]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[75]/strong[1]","endOffset":13},{"type":"TextPositionSelector","start":22518,"end":22531},{"type":"TextQuoteSelector","exact":"greedy action","prefix":"or state\n\n5.2 Greedy ActionsThe ","suffix":" is the action which has the hig"}]}]},{"text":"Q. What is an $\\epsilon$-greedy strategy in RL?\nA. Explore (e.g. randomly) with probability $\\epsilon$; exploit (i.e. take the greedy action) with probability $1-\\epsilon$.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[80]/strong[3]/span[1]/span[2]/span[1]/span[1]/math[1]/semantics[1]/mrow[1]/mi[1]","startOffset":0,"endContainer":"/div[1]/p[80]/strong[4]","endOffset":7},{"type":"TextPositionSelector","start":23542,"end":23560},{"type":"TextQuoteSelector","exact":"ϵ\\epsilonϵ﻿-greedy","prefix":"ibs/KaTeX/0.13.2/katex.min.css')","suffix":" (Greek lowercase letter epsilon"}]}]},{"text":"Q. How does an $\\epsilon$-greedy strategy apply differently in training vs. in performance?\nA. In performance, the agent should act purely greedily.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[84]","startOffset":0,"endContainer":"/div[1]/p[84]","endOffset":111},{"type":"TextPositionSelector","start":24166,"end":24277},{"type":"TextQuoteSelector","exact":"This is only used during training. If you want your agent to perform its best, you make it act purely greedily.","prefix":"bility (1−ϵ)with probability ϵ​\n","suffix":"A good value of @import url('htt"}]}]},{"text":"Q. When might lower vs. higher values of $\\epsilon$ in $\\epsilon$-greedy algorithms make sense?\nA. Higher values are better when there are relatively few states, since that will assure the agent will quickly visit them all; lower values are better when there are many states, but only very few with good reward.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[85]","startOffset":0,"endContainer":"/div[1]/p[85]","endOffset":141},{"type":"TextPositionSelector","start":24277,"end":24418},{"type":"TextQuoteSelector","exact":"A good value of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')ϵ\\epsilonϵ﻿ is different for different tasks.","prefix":"you make it act purely greedily.","suffix":" In small environments with few "}]}]},{"text":"Q. What is the value of a terminal state in a Markov decision process?\nA. $0$.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/div[4]/p[2]/strong[1]","startOffset":0,"endContainer":"/div[1]/div[4]/p[2]/span[1]/span[3]","endOffset":1},{"type":"TextPositionSelector","start":26420,"end":26539},{"type":"TextQuoteSelector","exact":"the value of any terminal state is @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')000﻿","prefix":"rn from this state.As a result, ","suffix":" since no rewards follow it.Let’"}]}]}]]