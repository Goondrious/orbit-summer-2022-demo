[[{"text":"Q. Notation for a Markov decision process's return at time $t$?\nA. $G_t$.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[15]","startOffset":0,"endContainer":"/div[1]/p[15]","endOffset":128},{"type":"TextPositionSelector","start":1879,"end":2007},{"type":"TextQuoteSelector","exact":"We define this as the return, denoted @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')GtG_tGt​﻿.","prefix":"ard it receives in the long run.","suffix":"\nSimple Return FormulaThe simple"}]}]},{"text":"Q. What does $G_t$ denote in a Markov decision process?\nA. The return at time $t$, i.e. the value it tries to maximize.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[15]/span[1]","startOffset":0,"endContainer":"/div[1]/p[15]/span[3]","endOffset":1},{"type":"TextPositionSelector","start":1879,"end":2007},{"type":"TextQuoteSelector","exact":"We define this as the return, denoted @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')GtG_tGt​﻿.","prefix":"ard it receives in the long run.","suffix":"\nSimple Return FormulaThe simple"}]}]},{"text":"Q. For a chess-playing Markov decision process, what is an \"episode\"?\nA. A single game of chess.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/div[1]/p[1]/strong[3]","startOffset":0,"endContainer":"/div[1]/div[1]/p[1]/strong[3]","endOffset":7},{"type":"TextPositionSelector","start":3330,"end":3337},{"type":"TextQuoteSelector","exact":"episode","prefix":"s are called episodic MDPs. The ","suffix":" ends when you reach those state"}]}]},{"text":"Q. Notation for the final time step in an episode Markov decision process?\nA. $T$.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/div[1]/p[2]","startOffset":10,"endContainer":"/div[1]/div[1]/p[2]","endOffset":139},{"type":"TextPositionSelector","start":3381,"end":3510},{"type":"TextQuoteSelector","exact":"this final timestep in an MDP is denoted by @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')TTT﻿ ","prefix":"ou reach those states.Typically ","suffix":"as in the equation above.E.g. in"}]}]},{"text":"Q. What does $T$ denote in the context of a Markov decision process?\nA. The final time step.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/div[1]/p[2]/span[1]","startOffset":0,"endContainer":"/div[1]/div[1]/p[2]/span[3]","endOffset":1},{"type":"TextPositionSelector","start":3381,"end":3510},{"type":"TextQuoteSelector","exact":"this final timestep in an MDP is denoted by @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')TTT﻿ ","prefix":"ou reach those states.Typically ","suffix":"as in the equation above.E.g. in"}]}]},{"text":"Q. What is an *episodic* Markov decision process?\nA. One with a terminal state, i.e. a finite number of time steps.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/div[1]/p[1]","startOffset":55,"endContainer":"/div[1]/div[1]/p[1]","endOffset":106},{"type":"TextPositionSelector","start":3273,"end":3324},{"type":"TextQuoteSelector","exact":"Those with terminal states are called episodic MDPs","prefix":"n infinite number of timesteps. ","suffix":". The episode ends when you reac"}]}]},{"text":"Q. What does $\\gamma$ denote in Markov decision processes?\nA. A discount factor for future rewards.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[27]","startOffset":17,"endContainer":"/div[1]/p[27]/span[1]/span[3]","endOffset":1},{"type":"TextPositionSelector","start":6602,"end":6723},{"type":"TextQuoteSelector","exact":"we introduce a discount factor, @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')γ\\gammaγ﻿","prefix":"r your rewards.To imitate this, ","suffix":" (lowercase Greek letter gamma) "}]}]},{"text":"Q. What range of values can $\\gamma$ take in Markov decision processes?\nA. $0 <= \\gamma <= 1$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[27]","startOffset":199,"endContainer":"/div[1]/p[27]/span[4]/span[3]","endOffset":1},{"type":"TextPositionSelector","start":6784,"end":6901},{"type":"TextQuoteSelector","exact":"We define @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')0≤γ≤10\\leq\\gamma\\leq10≤γ≤1﻿","prefix":"a) when calculating the return. ","suffix":", since gamma discounts the rela"}]}]},{"text":"Q. Give the general formula for a Markov decision process's return at time $t$.\nA. $G_t = \\sum_{i=1}^{\\infty}\\gamma^i r_{t+i}$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[35]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[35]/strong[2]","endOffset":6},{"type":"TextPositionSelector","start":8338,"end":8368},{"type":"TextQuoteSelector","exact":"general formula for the return","prefix":"mulaThis leads to the following ","suffix":":@import url('https://cdnjs.clou"}]}]},{"text":"Q. What trick do we use to tractably compute the return for Markov decision processes with infinite timesteps?\nA. Use a discount factor $\\gamma$ < 1. Because each step's return is scaled by $\\gamma^n$, future terms converge to zero. The resulting sum is effectively finite.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[33]","startOffset":43,"endContainer":"/div[1]/p[33]","endOffset":150},{"type":"TextPositionSelector","start":7945,"end":8052},{"type":"TextQuoteSelector","exact":" It allows us to solve MDPs that have infinite timesteps without having to consider infinite-sized returns.","prefix":"as a nice mathematical property.","suffix":" This is because the exponential"}]}]},{"text":"Q. Define $G_t^{\\text{Sum}}$ in terms of $G_t$.\nA. $G_t^{\\text{Sum}} = G_t$ where $\\gamma = 1$.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[36]/span[1]/span[2]/span[1]/span[1]/math[1]/semantics[1]/mrow[1]/msubsup[1]/mi[1]","startOffset":0,"endContainer":"/div[1]/p[36]","endOffset":113},{"type":"TextPositionSelector","start":8770,"end":8799},{"type":"TextQuoteSelector","exact":"GtSumG^{\\text{Sum}}_tGtSum​﻿ ","prefix":"ibs/KaTeX/0.13.2/katex.min.css')","suffix":"we saw earlier is a special case"}]}]},{"text":"Q. Define a Markov decision process's value function (in words).\nA. The expected return from a given state while following a given policy.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/blockquote[1]","startOffset":12,"endContainer":"/div[1]/blockquote[1]","endOffset":30},{"type":"TextPositionSelector","start":12041,"end":12059},{"type":"TextQuoteSelector","exact":"The value function","prefix":"vπ(s)v_\\pi(s)vπ​(s)﻿Definition: ","suffix":" @import url('https://cdnjs.clou"}]}]},{"text":"Q. Give the mathematical definition of a Markov decision process's value function.\nA. $v_\\pi(s_t) = \\mathbb{E}_\\pi[G_t]$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/figure[7]/div[1]/span[1]/span[1]/span[2]/span[1]/span[2]/span[1]","startOffset":0,"endContainer":"/div[1]/figure[7]/div[1]/span[1]/span[1]/span[2]/span[2]/span[5]","endOffset":1},{"type":"TextPositionSelector","start":14645,"end":14662},{"type":"TextQuoteSelector","exact":"vπ​(st​)=Eπ​[Gt​]","prefix":"pi(s_t) = \\mathbb{E}_{ \\pi}[G_t]","suffix":"NotationDefinitions of notation "}]}]},{"text":"Q. How can we use a Markov decision process's estimated value function to form a policy?\nA. Choose the action which leads to the successor state with the highest value according to the value function.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[58]","startOffset":164,"endContainer":"/div[1]/p[58]","endOffset":273},{"type":"TextPositionSelector","start":16336,"end":16445},{"type":"TextQuoteSelector","exact":"This allows it to determine its policy by choosing the action that leads to the highest value successor state","prefix":" experience in the environment. ","suffix":".@import url('https://cdnjs.clou"}]}]},{"text":"Q. What is a Markov decision process's state transition function?\nA. Given an action and a state, produces a successor state.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/ol[3]/li[1]/p[1]/strong[1]","startOffset":0,"endContainer":"/div[1]/ol[3]/li[1]/p[1]/strong[1]","endOffset":25},{"type":"TextPositionSelector","start":19009,"end":19034},{"type":"TextQuoteSelector","exact":"state transition function","prefix":"ral, this is referred to as the ","suffix":".The reward for every state-acti"}]}]},{"text":"Q. What is a Markov decision process's reward function?\nA. For every state and action, gives the associated reward.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/ol[4]/li[1]/p[1]/strong[1]","startOffset":0,"endContainer":"/div[1]/ol[4]/li[1]/p[1]/strong[1]","endOffset":15},{"type":"TextPositionSelector","start":19121,"end":19136},{"type":"TextQuoteSelector","exact":"reward function","prefix":"ral, this is referred to as the ","suffix":"\nThere aren't any terminal state"}]}]},{"text":"Q. How does the Bellman Equation help us solve Markov decision processes' value functions?\nA. It allows us to solve for the value function at a given state using only the successor state's value. No need to search the entire tree for each state.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[83]","startOffset":0,"endContainer":"/div[1]/p[83]","endOffset":128},{"type":"TextPositionSelector","start":20718,"end":20846},{"type":"TextQuoteSelector","exact":"Next we can use the Bellman Equation to calculate the values for the other states, using the value for Sleep we just calculated.","prefix":"tion_fn(state, action)\n\nEND FOR\n","suffix":"# `Pub` is 1 step from `Sleep` i"}]}]},{"text":"Q. Give the Bellman Equation (assuming deterministic rewards).\nA. $v_\\pi(s_t) = r_t + \\gamma v_\\pi(s_{t+1})$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[67]","startOffset":0,"endContainer":"/div[1]/p[67]","endOffset":64},{"type":"TextPositionSelector","start":18036,"end":18100},{"type":"TextQuoteSelector","exact":"Mathematically, when dealing with deterministic rewards this is:","prefix":"sitions & rewards are in green.\n","suffix":"@import url('https://cdnjs.cloud"}]}]}]]