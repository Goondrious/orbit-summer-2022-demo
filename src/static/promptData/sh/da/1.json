[[{"text":"Q. What kind of problems do reinforcement learning systems solve?\nA. Sequential decision-making problems (i.e. situations where an agent must repeatedly choose an action)","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[19]","startOffset":0,"endContainer":"/div[1]/p[19]","endOffset":72},{"type":"TextPositionSelector","start":2248,"end":2320},{"type":"TextQuoteSelector","exact":"Reinforcement Learning problems are sequential decision-making problems.","prefix":"f problems & a set of solutions.","suffix":" I.e. you must repeatedly make d"}]}]},{"text":"Q. How does the data used to train RL systems differ from supervised and unsupervised learning?\nA. Supervised/unsupervised systems learn from a dataset; RL systems learn from feedback given by an environment.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/ul[1]/li[1]/details[1]/p[4]","startOffset":24,"endContainer":"/div[1]/ul[1]/li[1]/details[1]/p[4]","endOffset":159},{"type":"TextPositionSelector","start":3279,"end":3414},{"type":"TextQuoteSelector","exact":"RL differs from both supervised learning and unsupervised learning in that it does not have access to an unlabelled or labelled dataset","prefix":" groups.Reinforcement Learning: ","suffix":", but rather selects actions and"}]}]},{"text":"Q. Give two examples of problems RL could solve.\nA. e.g. choosing moves to make in a board game; adjusting the shower knobs to maintain a comfortable temperature; a search-and-rescue robot deciding how to navigate a building","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/h3[2]","startOffset":0,"endContainer":"/div[1]/p[34]","endOffset":0},{"type":"TextPositionSelector","start":4642,"end":4650},{"type":"TextQuoteSelector","exact":"Examples","prefix":"mber of practical applications!\n","suffix":"A good way to understand reinfor"}]}]},{"text":"Q. What's the technical name for the problems which RL algorithms solve?\nA. Markov decision processes.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[42]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[42]/strong[1]","endOffset":25},{"type":"TextPositionSelector","start":5765,"end":5790},{"type":"TextQuoteSelector","exact":"Markov Decision Processes","prefix":")\n\n\n3. Markov Decision Processes","suffix":" is the technical name for the b"}]}]},{"text":"Q. How do Markov decision processes model time?\nA. As a series of discrete, fixed-size steps.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[43]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[43]/strong[1]","endOffset":23},{"type":"TextPositionSelector","start":5897,"end":5920},{"type":"TextQuoteSelector","exact":"discrete-time processes","prefix":"rning algorithms solve.They are ","suffix":". This means time progresses in "}]}]},{"text":"Q. What three key elements occur at each time step in a Markov decision process?\nA. Agent takes action $a$; the environment state $s$ is updated; a reward $r$ is given.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[44]","startOffset":0,"endContainer":"/div[1]/p[44]","endOffset":454},{"type":"TextPositionSelector","start":6044,"end":6498},{"type":"TextQuoteSelector","exact":"At each timestep @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')ttt﻿, the agent takes an action @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')aaa﻿, the state @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')sss﻿ updates as a result of this action and a reward, @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')rrr﻿, is given.","prefix":"nds, so at 0.1s, 0.2s, 0.3s, ...","suffix":"\nThey have specific definitions "}]}]},{"text":"Q. What defines the optimal solution to a Markov decision process?\nA. It maximizes the sum of future rewards.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[48]","startOffset":72,"endContainer":"/div[1]/p[48]/strong[1]","endOffset":21},{"type":"TextPositionSelector","start":7110,"end":7132},{"type":"TextQuoteSelector","exact":" sum of future rewards","prefix":"way of acting that maximises the","suffix":". I.e. the reward at every futur"}]}]},{"text":"Q. In this course, what's meant by the notation $x_t$?\nA. The value of $x$ at time $t$.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[55]","startOffset":0,"endContainer":"/div[1]/p[55]","endOffset":317},{"type":"TextPositionSelector","start":7435,"end":7752},{"type":"TextQuoteSelector","exact":"Note: the subscript @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')ttt﻿ in @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')sts_tst​﻿ just means it's the state at time @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')ttt﻿.","prefix":"/0.13.2/katex.min.css')sts_tst​﻿","suffix":" We'll follow this convention th"}]}]},{"text":"Q. What does it mean that Markov decision processes are memoryless?\nA. The current state fully defines the environment—i.e. previous states and actions provide no extra information.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[56]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[56]/strong[1]","endOffset":10},{"type":"TextPositionSelector","start":8013,"end":8023},{"type":"TextQuoteSelector","exact":"memoryless","prefix":"Decision Process is that it is '","suffix":"'.This means that the current st"}]}]},{"text":"Q. If you wanted to losslessly save and restore a Markov decision process, what information would you need to persist?\nA. Only the current state.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[58]","startOffset":17,"endContainer":"/div[1]/p[58]","endOffset":97},{"type":"TextPositionSelector","start":8229,"end":8309},{"type":"TextQuoteSelector","exact":"if you knew only this, you could recreate the game completely from this point on","prefix":"s (or actions).Put a third way, ","suffix":".Examples:The pieces and their l"}]}]},{"text":"Q. In a Markov decision process modeling a search and rescue robot, what might the state describe?\nA. e.g. Estimates of the robot's pose and the building's layout.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[59]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[59]/strong[1]","endOffset":8},{"type":"TextPositionSelector","start":8310,"end":8318},{"type":"TextQuoteSelector","exact":"Examples","prefix":"e completely from this point on.","suffix":":The pieces and their locations "}]}]},{"text":"Q. What is a \"successor state\" in a Markov decision process?\nA. The agent's next state (after taking its next action).","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[63]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[63]/strong[1]","endOffset":15},{"type":"TextPositionSelector","start":8649,"end":8664},{"type":"TextQuoteSelector","exact":"successor state","prefix":"kes one action.These affect the ","suffix":" of the environment - the next s"}]}]},{"text":"Q. If a Markov decision process takes only one action per time step, how to model a search-and-rescue robot which can both accelerate and turn?\nA. Model changes to acceleration and orientation as independent dimensions of a single action vector.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[64]","startOffset":0,"endContainer":"/div[1]/p[64]","endOffset":48},{"type":"TextPositionSelector","start":8712,"end":8760},{"type":"TextQuoteSelector","exact":"Action can have multiple independent dimensions.","prefix":"ent - the next state you are in.","suffix":" E.g. a robot can decide its acc"}]}]},{"text":"Q. Why might it not be the best strategy for an RL agent to simply choose the next action with the highest estimate reward?\nA. Its goal is to maximize the total reward over all future steps, not just the next one.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[69]/strong[2]","startOffset":0,"endContainer":"/div[1]/p[69]/strong[4]","endOffset":17},{"type":"TextPositionSelector","start":9291,"end":9346},{"type":"TextQuoteSelector","exact":"maximize the total reward it receives over the long run","prefix":"ent.The agent’s objective is to ","suffix":".\nThe reward is received as the "}]}]},{"text":"Q. Practically speaking, how does an RL designer express different goals for his agents?\nA. By defining different reward functions.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[74]","startOffset":16,"endContainer":"/div[1]/p[74]","endOffset":100},{"type":"TextPositionSelector","start":11008,"end":11092},{"type":"TextQuoteSelector","exact":"by defining the reward function, you can train your agent to achieve different goals","prefix":" agent to solve.This means that ","suffix":". For example, in chess, if you "}]}]},{"text":"Q. How might you cause a robot trained via RL to find the quickest path to its destination?\nA. e.g. Give a negative reward for every time step before it reaches its destination.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/ol[9]/li[1]/code[1]","startOffset":0,"endContainer":"/div[1]/ol[9]/li[1]","endOffset":50},{"type":"TextPositionSelector","start":11317,"end":11367},{"type":"TextQuoteSelector","exact":"+100 for reaching the goal location, -1 otherwise*","prefix":"game, -1 for losing, 0 otherwise","suffix":"@import url('https://cdnjs.cloud"}]}]},{"text":"Q. What is a \"policy\" in an RL system?\nA. A function which chooses an action to take, given a state.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[88]","startOffset":0,"endContainer":"/div[1]/p[88]","endOffset":74},{"type":"TextPositionSelector","start":12130,"end":12204},{"type":"TextQuoteSelector","exact":"A policy is a function that picks which action to take when in each state.","prefix":"y for this tutorial, I promise!)","suffix":"It’s the core of a reinforcement"}]}]},{"text":"Q. In a chess-playing RL system, what might the policy function take as input and produce as output?\nA. The input is the state, i.e. the positions of all pieces on the board. The output is the action, i.e. a move to make.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[90]","startOffset":188,"endContainer":"/div[1]/p[90]","endOffset":232},{"type":"TextPositionSelector","start":12485,"end":12529},{"type":"TextQuoteSelector","exact":"takes a state as input and outputs an action","prefix":"hematically, it’s a policy that ","suffix":"ExampleIn the case of a game of "}]}]},{"text":"Q. What does it mean for a policy function to be stochastic?\nA. It outputs a probability for each action, rather than defining a single definite action.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[92]","startOffset":12,"endContainer":"/div[1]/p[92]/em[1]","endOffset":10},{"type":"TextPositionSelector","start":12928,"end":12954},{"type":"TextQuoteSelector","exact":"policies may be stochastic","prefix":"n in this sequence).In general, ","suffix":", specifying probabilities for e"}]}]},{"text":"Q. In a Markov decision process, action $a_t$ produces reward $r$ at what time step?\nA. $t+1$, i.e. $a_t$ yields $r_{t+1}$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[60]","startOffset":0,"endContainer":"/div[1]/p[60]","endOffset":69},{"type":"TextPositionSelector","start":9528,"end":9597},{"type":"TextQuoteSelector","exact":"The reward is received as the environment transitions between states.","prefix":" it receives over the long run.\n","suffix":" So the sequence of events when "}]}]}]]