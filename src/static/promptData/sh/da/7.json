[[{"text":"Q. How might you use a neural network in the context of RL?\nA. e.g. to approximate a value function","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[10]","startOffset":258,"endContainer":"/div[1]/p[10]","endOffset":287},{"type":"TextPositionSelector","start":2567,"end":2596},{"type":"TextQuoteSelector","exact":"We can take advantage of this","prefix":"nown as function approximation. ","suffix":".\nIn most reinforcement learning"}]}]},{"text":"Q. Why are neural networks often suitable for approximating an RL value function? (give at least two reasons)\nA. e.g. they can represent any function; they can be efficiently updated with new data; their learned functions can generalize well to unseen points; they're fast to evaluate; they're relatively easy to train","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[20]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[20]/strong[1]","endOffset":32},{"type":"TextPositionSelector","start":4151,"end":4183},{"type":"TextQuoteSelector","exact":"They have the following benefits","prefix":"rning method we’re looking for?\n","suffix":":Representational Capacity: they"}]}]},{"text":"Q. What values are learned when training a typical neural network?\nA. The weights and biases.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/ol[7]/li[1]","startOffset":128,"endContainer":"/div[1]/ol[7]/li[1]","endOffset":161},{"type":"TextPositionSelector","start":6825,"end":6858},{"type":"TextQuoteSelector","exact":"This is another learned parameter","prefix":"﻿ is added to the weighted sum. ","suffix":". This is optional.An ‘activatio"}]}]},{"text":"Q. Give the mathematical formula for the output of a typical neuron.\nA. $g(b + \\sum_{i=1}^N x_iw_i)$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[43]","startOffset":0,"endContainer":"/div[1]/p[43]","endOffset":25},{"type":"TextPositionSelector","start":7968,"end":7993},{"type":"TextQuoteSelector","exact":"As a mathematical formula","prefix":"t)g(⋅)﻿ below (explained below).","suffix":" which generalises to any number"}]}]},{"text":"Q. What does $g(\\cdot)$ denote in the context of neural networks?\nA. The activation function.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[42]","startOffset":77,"endContainer":"/div[1]/p[42]","endOffset":191},{"type":"TextPositionSelector","start":7835,"end":7949},{"type":"TextQuoteSelector","exact":"denoted by @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')g(⋅)g(\\cdot)g(⋅)﻿ below","prefix":"hrough the activation function, ","suffix":" (explained below).As a mathemat"}]}]},{"text":"Q. Verbally describe how a neuron computes its output (three steps)\nA. It computes the weighted sum of its inputs ($\\sum_i w_i x_i$), adds a bias ($b$), then passes the sum through the activation function ($g(\\cdot)$).","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[43]/span[1]","startOffset":0,"endContainer":"/div[1]/p[43]/span[1]","endOffset":25},{"type":"TextPositionSelector","start":7968,"end":7993},{"type":"TextQuoteSelector","exact":"As a mathematical formula","prefix":"t)g(⋅)﻿ below (explained below).","suffix":" which generalises to any number"}]}]},{"text":"Q. Why are neural network activation functions typically non-linear?\nA. This allows the network to approximate non-linear functions.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[46]","startOffset":46,"endContainer":"/div[1]/p[46]","endOffset":98},{"type":"TextPositionSelector","start":9004,"end":9056},{"type":"TextQuoteSelector","exact":"They introduce non-linearity into the neural network","prefix":"ctions are nonlinear functions. ","suffix":", allowing it to represent more "}]}]},{"text":"Q. Define the ReLU activation function.\nA. $g(x)=\\text{max}(0,x)$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[50]","startOffset":88,"endContainer":"/div[1]/p[50]/strong[4]","endOffset":19},{"type":"TextPositionSelector","start":9864,"end":9936},{"type":"TextQuoteSelector","exact":"It outputs 0 for negative inputs and the input value for positive inputs","prefix":"or now is the ReLU (top right). ","suffix":".\n\n\n7.3 Mini-exercises: Adam’s A"}]}]},{"text":"Q. What are the inputs to a neuron in $h_n$?\nA. A vector representing the outputs of all the neurons in $h_{n-1}$. $h_1$'s inputs are $i$'s outputs.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[71]","startOffset":74,"endContainer":"/div[1]/p[71]","endOffset":320},{"type":"TextPositionSelector","start":12229,"end":12475},{"type":"TextQuoteSelector","exact":"Every neuron in layer @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')h2h_2h2​﻿ takes each number output from layer @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')h1h_1h1​﻿ as input","prefix":"neuron outputs a single number. ","suffix":". So every individual neuron in "}]}]},{"text":"Q. Typical notation for the input layer, $L$ hidden layers, and output layer?\nA. $i$, $h_1 \\ldots h_L$, $o$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[70]","startOffset":0,"endContainer":"/div[1]/p[70]","endOffset":223},{"type":"TextPositionSelector","start":11356,"end":11579},{"type":"TextQuoteSelector","exact":"In the diagram, there are @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')n1n_1n1​﻿ neurons in layer @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')h1h_1h1​﻿.","prefix":" layer and outputting a number.\n","suffix":" Every neuron in layer @import u"}]}]},{"text":"Q. If we use a neural network to approximate $y=f(x)$, how do we input $x$ and where do we read $y$?\nA. Provide $x$ as input to the input layer $i$; the outputs of the output layer $o$ represent $y$.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[72]","startOffset":0,"endContainer":"/div[1]/p[72]","endOffset":53},{"type":"TextPositionSelector","start":12714,"end":12767},{"type":"TextQuoteSelector","exact":"We read out the values from the output layer directly","prefix":"css')n1n_1n1​﻿ input dimensions.","suffix":" - no other neurons follow them."}]}]},{"text":"Q. If a hidden layer has a 4-dimensional input and a 2-dimensional output, what do we know about the architecture of the network?\nA. The preceding layer has 4 neurons; the subsequent layer has 2 neurons.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[77]","startOffset":0,"endContainer":"/div[1]/p[77]/span[1]","endOffset":31},{"type":"TextPositionSelector","start":13013,"end":13077},{"type":"TextQuoteSelector","exact":"When we talk about the dimensions, we mean the number of numbers","prefix":"t neural networks.\n\n\n\nDimensions","suffix":". You can think of this as the l"}]}]}]]