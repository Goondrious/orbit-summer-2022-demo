[[{"text":"Q. How might you use a neural network in the context of RL?\nA. e.g. to approximate a value function","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[10]","startOffset":258,"endContainer":"/div[1]/p[10]","endOffset":287},{"type":"TextPositionSelector","start":2567,"end":2596},{"type":"TextQuoteSelector","exact":"We can take advantage of this","prefix":"nown as function approximation. ","suffix":".\nIn most reinforcement learning"}]}]},{"text":"Q. Why are neural networks often suitable for approximating an RL value function? (give at least two reasons)\nA. e.g. 1. they can represent any function; 2. they can be updated with new data; 3. they can generalize well to unseen states; 4. they're fast to evaluate; 5. they're relatively easy to train","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[20]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[20]/strong[1]","endOffset":32},{"type":"TextPositionSelector","start":4151,"end":4183},{"type":"TextQuoteSelector","exact":"They have the following benefits","prefix":"rning method weâ€™re looking for?\n","suffix":":Representational Capacity: they"}]}]},{"text":"Q. What parameters are learned when training a typical neural network?\nA. The weights and biases.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/ol[7]/li[1]","startOffset":128,"endContainer":"/div[1]/ol[7]/li[1]","endOffset":161},{"type":"TextPositionSelector","start":6825,"end":6858},{"type":"TextQuoteSelector","exact":"This is another learned parameter","prefix":"ï»¿ is added to the weighted sum. ","suffix":". This is optional.An â€˜activatio"}]}]},{"text":"Q. Verbally describe how a neuron computes its output (three steps)\nA. It computes the weighted sum of its inputs ($\\sum_i w_i x_i$), adds a bias ($b$), then passes the sum through the activation function ($g(\\cdot)$).","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[43]/span[1]","startOffset":0,"endContainer":"/div[1]/p[43]/span[1]","endOffset":25},{"type":"TextPositionSelector","start":7968,"end":7993},{"type":"TextQuoteSelector","exact":"As a mathematical formula","prefix":"t)g(â‹…)ï»¿ below (explained below).","suffix":" which generalises to any number"}]}]},{"text":"Q. Why are neural network activation functions typically non-linear?\nA. This allows the network to approximate non-linear functions.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[46]","startOffset":46,"endContainer":"/div[1]/p[46]","endOffset":98},{"type":"TextPositionSelector","start":9004,"end":9056},{"type":"TextQuoteSelector","exact":"They introduce non-linearity into the neural network","prefix":"ctions are nonlinear functions. ","suffix":", allowing it to represent more "}]}]},{"text":"Q. Define the ReLU activation function.\nA. $g(x)=\\text{max}(0,x)$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[50]","startOffset":88,"endContainer":"/div[1]/p[50]/strong[4]","endOffset":19},{"type":"TextPositionSelector","start":9864,"end":9936},{"type":"TextQuoteSelector","exact":"It outputs 0 for negative inputs and the input value for positive inputs","prefix":"or now is the ReLU (top right). ","suffix":".\n\n\n7.3 Mini-exercises: Adamâ€™s A"}]}]},{"text":"Q. What are the inputs to a neuron in $h_n$?\nA. A vector representing the outputs of all the neurons in $h_{n-1}$. $h_1$'s inputs are $i$'s outputs.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[71]","startOffset":74,"endContainer":"/div[1]/p[71]","endOffset":320},{"type":"TextPositionSelector","start":12229,"end":12475},{"type":"TextQuoteSelector","exact":"Every neuron in layer @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')h2h_2h2â€‹ï»¿ takes each number output from layer @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')h1h_1h1â€‹ï»¿ as input","prefix":"neuron outputs a single number. ","suffix":". So every individual neuron in "}]}]},{"text":"Q. If we use a neural network to approximate $y=f(x)$, how do we input $x$ and where do we read $y$?\nA. Provide $x$ as input to the input layer $i$; the outputs of the output layer $o$ represent $y$.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[72]","startOffset":0,"endContainer":"/div[1]/p[72]","endOffset":53},{"type":"TextPositionSelector","start":12714,"end":12767},{"type":"TextQuoteSelector","exact":"We read out the values from the output layer directly","prefix":"css')n1n_1n1â€‹ï»¿ input dimensions.","suffix":" - no other neurons follow them."}]}]},{"text":"Q. If a hidden layer has a 4-dimensional input and a 2-dimensional output, what do we know about the architecture of the network?\nA. The preceding layer has 4 neurons; the subsequent layer has 2 neurons.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[77]","startOffset":0,"endContainer":"/div[1]/p[77]/span[1]","endOffset":31},{"type":"TextPositionSelector","start":13013,"end":13077},{"type":"TextQuoteSelector","exact":"When we talk about theÂ dimensions, we mean the number of numbers","prefix":"t neural networks.\n\n\n\nDimensions","suffix":". You can think of this as the l"}]}]},{"text":"Q. If a hidden layer neuron has 4 input dimensions and 2 output dimensions, what do we know about the architecture of the network?\nA. The preceding layer has 4 neurons; the subsequent layer has 2 neurons.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[77]","startOffset":0,"endContainer":"/div[1]/p[77]/span[1]","endOffset":31},{"type":"TextPositionSelector","start":13013,"end":13077},{"type":"TextQuoteSelector","exact":"When we talk about theÂ dimensions, we mean the number of numbers","prefix":"t neural networks.\n\n\n\nDimensions","suffix":". You can think of this as the l"}]}]},{"text":"Q. What properties define the architecture of a neural network? (give at least two)\nA. e.g. the number of layers, the number of neurons in each layer, the activation function","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[83]","startOffset":63,"endContainer":"/div[1]/p[83]","endOffset":203},{"type":"TextPositionSelector","start":13800,"end":13940},{"type":"TextQuoteSelector","exact":"The number of layers, number of neurons in each layer and the activation functions used all make up the 'architecture' of the neural network","prefix":"e design of the neural network. ","suffix":".Certain architectures are well-"}]}]},{"text":"Q. What properties must a neuron's activation functions have? (give at least 2)\nA. e.g. non-linearity, differentiability","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[49]","startOffset":6,"endContainer":"/div[1]/p[49]/strong[2]","endOffset":14},{"type":"TextPositionSelector","start":9591,"end":9665},{"type":"TextQuoteSelector","exact":"another requirement of activation functions is that theyâ€™re differentiable","prefix":"sing through the function.Note: ","suffix":". This just means that they donâ€™"}]}]},{"text":"Q. How does Delta Academy suggest you approach designing a neural network's architecture?\nA. Start with a simple architecture, then make it more complex until performance stops improving. Or copy a network which solved a similar problem.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[86]/span[1]","startOffset":0,"endContainer":"/div[1]/p[86]","endOffset":104},{"type":"TextPositionSelector","start":14479,"end":14572},{"type":"TextQuoteSelector","exact":"a good approach to architecture design is to first train a network with a simple architecture","prefix":" connections drawn ðŸ˜²Typically, ","suffix":". Then iterate on it - making it"}]}]},{"text":"Q. Why does Delta Academy suggest starting with simple neural network architectures?\nA. Spot bugs quickly; make sure the model is learning what you intend. Then it's easy to scale up iteratively.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[88]","startOffset":202,"endContainer":"/div[1]/p[88]","endOffset":339},{"type":"TextPositionSelector","start":15445,"end":15582},{"type":"TextQuoteSelector","exact":"The key is making sure the model is learning what you want it to, that you know how to spot when that isnâ€™t the case and how to debug it!","prefix":"and more neurons in each layer. ","suffix":"\n\n7.5 Wrapping up & ReviewThis T"}]}]},{"text":"Q. Why use a neural network to learn features for reinforcement learning, instead of designing them by hand?\nA. It requires deep expert knowledge of the problem.\n\nIt's hard to design a set of features which can reliably differentiate between states with different values.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[7]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[7]","endOffset":80},{"type":"TextPositionSelector","start":1995,"end":2046},{"type":"TextQuoteSelector","exact":"learning features instead of designing them by hand","prefix":"cy.Can we solve these issues by ","suffix":"?\nFunction Approximation and Mac"}]}]}]]