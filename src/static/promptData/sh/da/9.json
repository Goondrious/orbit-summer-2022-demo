[[{"text":"Q. How to create a mean squared error loss function in PyTorch?\nA. `torch.nn.MSELoss()`","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/h3[1]/code[1]/strong[1]","startOffset":0,"endContainer":"/div[1]/h3[1]/code[1]/strong[1]","endOffset":10},{"type":"TextPositionSelector","start":2038,"end":2048},{"type":"TextQuoteSelector","exact":"nn.MSELoss","prefix":"for all torch's loss functions)\n","suffix":"This is the Mean-Squared Error l"}]}]},{"text":"Q. Given `loss = torch.nn.MSELoss()`, calculate the loss for target values `t` and neural network outputs `o`.\nA. `loss(o, t)`","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/pre[1]/code[1]","startOffset":351,"endContainer":"/div[1]/pre[1]/code[1]","endOffset":358},{"type":"TextPositionSelector","start":2894,"end":2901},{"type":"TextQuoteSelector","exact":"loss_fn","prefix":"wo - should equal 0.0111\nloss = ","suffix":"(predicted_heights, true_heights"}]}]},{"text":"Q. In PyTorch, what does calling `loss.backward()` on a computed loss value do?\nA. It computes and updates internal gradient values associated with the parameters (e.g. neural network weights) used to produce that loss.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[33]","startOffset":0,"endContainer":"/div[1]/p[33]","endOffset":114},{"type":"TextPositionSelector","start":4905,"end":5019},{"type":"TextQuoteSelector","exact":"It doesn't actually change the neural network parameters, just calculates the directions to change the parameters.","prefix":"o arguments and returns nothing.","suffix":"data = torch.tensor([[1.66], [1."}]}]},{"text":"Q. How does stochastic gradient descent differ from true gradient descent?\nA. It approximates the true gradient of the loss function by evaluating the loss function's gradient on a finite set of data points.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[38]","startOffset":111,"endContainer":"/div[1]/p[38]","endOffset":297},{"type":"TextPositionSelector","start":5625,"end":5811},{"type":"TextQuoteSelector","exact":"It’s an approximate form of gradient descent which replaces the true gradient with the gradient calculated from the loss function on a finite set of datapoints for the current parameters","prefix":"at is used in machine learning. ","suffix":".To do true gradient descent…We "}]}]},{"text":"Q. Why can't we use true gradient descent in RL (and must use e.g. SGD instead)?\nA. We have a finite set of estimates of the value function, sampled from interacting with the environment. To find the true gradient, we'd need the true values of all states for the current parameters.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/ul[2]/li[1]/details[1]/p[3]","startOffset":0,"endContainer":"/div[1]/ul[2]/li[1]/details[1]/p[3]","endOffset":97},{"type":"TextPositionSelector","start":6088,"end":6185},{"type":"TextQuoteSelector","exact":"Instead, we only have the performance on a finite number of datapoints for the current parameters","prefix":"the true gradient at this point.","suffix":", sampled from interacting with "}]}]},{"text":"Q. Given a PyTorch neural network `nn`, initialize an SGD optimizer with a reasonable learning rate.\nA. `torch.optim.SGD(nn.parameters(), lr=1e-3)`","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[40]","startOffset":42,"endContainer":"/div[1]/p[40]","endOffset":53},{"type":"TextPositionSelector","start":6276,"end":6287},{"type":"TextQuoteSelector","exact":"2 arguments","prefix":"o initialise the optimizer with ","suffix":":Neural network parameters (neur"}]}]},{"text":"Q. When must we call `zero_grad()` on a PyTorch optimizer in a typical ML scenario? Why?\nA. Before calling `backward()` on the value being optimized (e.g. the loss). `zero_grad()` resets the internal gradient values associated with the model parameters, so that they can be computed anew in each iteration.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[43]","startOffset":0,"endContainer":"/div[1]/p[43]","endOffset":78},{"type":"TextPositionSelector","start":6541,"end":6619},{"type":"TextQuoteSelector","exact":"This resets the gradients to 0 used to calculate the new parameter update step","prefix":", lr=1e-3)\noptimizer.zero_grad()","suffix":". This is important to do before"}]}]},{"text":"Q. What does calling `step()` on a PyTorch optimizer do?\nA. It updates the model parameters using the internal gradients computed through backpropagation.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/h3[5]/code[1]/strong[1]","startOffset":0,"endContainer":"/div[1]/h3[5]/code[1]/strong[1]","endOffset":16},{"type":"TextPositionSelector","start":6745,"end":6761},{"type":"TextQuoteSelector","exact":"optimizer.step()","prefix":"ackward().optimizer.zero_grad()\n","suffix":"¶This actually performs 1 update"}]}]},{"text":"Q. What are the five steps of a basic PyTorch SGD training loop?\nA. Compute the model's output on some training data; calculate the loss from the labels and predictions; reset the gradients  with `optimizer.zero_grad()`; calculate the gradients via backpropagation with `loss.backward()`; update the model parameters with `optimizer.step()`.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/ol[7]/li[1]","startOffset":0,"endContainer":"/div[1]/ol[7]/li[1]","endOffset":13},{"type":"TextPositionSelector","start":8130,"end":8143},{"type":"TextQuoteSelector","exact":"Training loop","prefix":"ialize the network and optimizer","suffix":":Make predictions on some traini"}]}]},{"text":"Q. How do PyTorch loss functions behave by default when given multiple data points?\nA. They compute the loss for each data point, then average.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[58]","startOffset":100,"endContainer":"/div[1]/p[58]","endOffset":143},{"type":"TextPositionSelector","start":10201,"end":10244},{"type":"TextQuoteSelector","exact":"you’re averaging the gradient over them all","prefix":"e loss for multiple datapoints, ","suffix":". Not every loss from every data"}]}]},{"text":"Q. Why run stochastic gradient descent over more than one data point at once?\nA. Averaging the loss over many data points will reduce noise. The gradient from just one data point can produce instability.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[58]","startOffset":145,"endContainer":"/div[1]/p[58]","endOffset":215},{"type":"TextPositionSelector","start":10246,"end":10316},{"type":"TextQuoteSelector","exact":"Not every loss from every datapoint will give you the ‘right’ gradient","prefix":"ing the gradient over them all. ","suffix":", that’s why it’s Stochastic Gra"}]}]},{"text":"Q. Why not run stochastic gradient descent over as many data points as possible?\nA. Excessively large batches will slow down training without appreciably improving the gradient estimates.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[60]","startOffset":0,"endContainer":"/div[1]/p[60]","endOffset":125},{"type":"TextPositionSelector","start":10575,"end":10700},{"type":"TextQuoteSelector","exact":"You get diminishing returns in how good the approximation is of the gradient, while the cost to compute the gradient goes up.","prefix":"s and lots? How about 1,000,000?","suffix":"So there’s a tradeoff in the ‘ba"}]}]},{"text":"Q. Give the mathematical definition of the TD Learning-derived loss function for approximating RL value functions, in terms of a single step's $r$, $\\hat v_{\\boldsymbol{\\theta}}$, and $\\gamma$.\nA. $\\text{Loss}^{\\text{TD}}(\\boldsymbol{\\theta}) = [r_{t+1} + \\gamma \\hat v_{\\boldsymbol{\\theta}}(s_{t+1}) - \\hat  v_{\\boldsymbol{\\theta}}(s_t)]^2$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[85]","startOffset":14,"endContainer":"/div[1]/p[85]","endOffset":59},{"type":"TextPositionSelector","start":15621,"end":15666},{"type":"TextQuoteSelector","exact":"the following definition of the loss function","prefix":"t+1​+γv^θ​(st+1​)\nThis leads to ","suffix":" that can be used practically in"}]}]},{"text":"Q. Explain the three terms of the TD Learning-derived loss function, $\\text{Loss}^{\\text{TD}}(\\boldsymbol{\\theta}) = [r_{t+1} + \\gamma \\hat v_{\\boldsymbol{\\theta}}(s_{t+1}) - \\hat  v_{\\boldsymbol{\\theta}}(s_t)]^2$\nA. $r_{t+1} + \\gamma \\hat v_{\\boldsymbol{\\theta}}(s_{t+1})$ approximates the true value of the current state $v_\\pi(s_t)$, using the TD update equation. $\\hat  v_{\\boldsymbol{\\theta}}(s_t)$ represents the model's output for the current state's value.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[85]/span[1]","startOffset":0,"endContainer":"/div[1]/p[85]","endOffset":60},{"type":"TextPositionSelector","start":15621,"end":15667},{"type":"TextQuoteSelector","exact":"the following definition of the loss function ","prefix":"t+1​+γv^θ​(st+1​)\nThis leads to ","suffix":"that can be used practically in "}]}]},{"text":"Q. When approximating an RL value function with a neural network, why can't we use the true loss function, $\\text{Loss}(\\boldsymbol{\\theta}) = \\sum_{s\\in\\mathcal{S}}\\mu(s)\\left[ v_\\pi(s) - \\hat{v}_{\\boldsymbol{\\theta}}(s) \\right]^2$\nA. We don't know the true value function, $v_\\pi(s)$.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[83]","startOffset":0,"endContainer":"/div[1]/p[83]","endOffset":53},{"type":"TextPositionSelector","start":14978,"end":15031},{"type":"TextQuoteSelector","exact":"Since we don’t have access to the true value function","prefix":"r value function approximator.\n\n","suffix":", @import url('https://cdnjs.clo"}]}]},{"text":"Q. Describe the Experience Replay technique for RL.\nA. Store the last $N$ data points; at each step, randomly choose some subset of size $M < N$; use those for training.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[104]","startOffset":0,"endContainer":"/div[1]/p[104]","endOffset":536},{"type":"TextPositionSelector","start":20363,"end":20899},{"type":"TextQuoteSelector","exact":"Experience replay stores @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')NNN﻿ datapoint tuples @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')(st,rt+1,st+1)(s_t, r_{t+1}, s_{t+1})(st​,rt+1​,st+1​)﻿ in a memory store @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')D\\mathcal{D}D﻿. We then sample @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')MMM﻿ datapoints to train on each time we perform an update step.","prefix":"etween two undesirable extremes.","suffix":" We sample from a uniform distri"}]}]},{"text":"Q. What values are stored for each data point in Experience Replay?\nA. $s_t$, $r_{t+1}$, $s_{t+1}$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[104]/span[4]/span[2]/span[1]/span[1]/math[1]/semantics[1]/mrow[1]/mo[1]/span[1]","startOffset":0,"endContainer":"/div[1]/p[104]/span[4]/span[3]/span[1]","endOffset":1},{"type":"TextPositionSelector","start":20570,"end":20625},{"type":"TextQuoteSelector","exact":"(st,rt+1,st+1)(s_t, r_{t+1}, s_{t+1})(st​,rt+1​,st+1​)﻿","prefix":"ibs/KaTeX/0.13.2/katex.min.css')","suffix":" in a memory store @import url('"}]}]},{"text":"Q. In RL, what's the problem with forming training batches from the last $N$ data points of experience?\nA. Those points are often highly correlated, since they represent adjacent steps in state space. You risk training the model well for \"nearby\" states but poorly for more \"distant\" ones.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[101]","startOffset":88,"endContainer":"/div[1]/p[101]","endOffset":125},{"type":"TextPositionSelector","start":19358,"end":19395},{"type":"TextQuoteSelector","exact":"The data you see is highly correlated","prefix":"this in reinforcement learning. ","suffix":": since it often takes many step"}]}]},{"text":"Q. Why shouldn't we use large values of $N$ in Experience Replay?\nA. Older data points may represent different policies (because we update the value function at each step).","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[111]","startOffset":0,"endContainer":"/div[1]/p[111]","endOffset":37},{"type":"TextPositionSelector","start":23424,"end":23461},{"type":"TextQuoteSelector","exact":"So we shouldn’t use an enormous value","prefix":"in.css')a1a_1a1​﻿ is incorrect.\n","suffix":" of @import url('https://cdnjs.c"}]}]},{"text":"Q. Empirically, in Experience Replay, what ratio of $N$ to $M$ tends to work well?\nA. 8:1 – 40:1","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[112]","startOffset":13,"endContainer":"/div[1]/p[112]","endOffset":362},{"type":"TextPositionSelector","start":24105,"end":24454},{"type":"TextQuoteSelector","exact":"we’ve found that values of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')NNN﻿ that are @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')8→40×8 \\rightarrow 40\\times8→40×﻿ larger than @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')MMM﻿ tend to work well","prefix":" another trade-off.Empirically, ","suffix":".ExampleIn the next exercise, we"}]}]}]]