[[{"text":"Q. Term for all possible states of a Markov decision process?\nA. The state space.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[2]/strong[2]","startOffset":0,"endContainer":"/div[1]/p[2]/strong[2]","endOffset":11},{"type":"TextPositionSelector","start":1209,"end":1220},{"type":"TextQuoteSelector","exact":"State Space","prefix":"l possible states in an MDP the ","suffix":".In recent years we've seen RL t"}]}]},{"text":"Q. What's the \"curse of dimensionality\" (with respect to RL)?\nA. The size of a state space increases very rapidly (exponentially) with the number of dimensions.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/ul[1]/li[1]/details[1]/summary[1]/strong[3]","startOffset":0,"endContainer":"/div[1]/ul[1]/li[1]/details[1]/summary[1]/strong[3]","endOffset":23},{"type":"TextPositionSelector","start":5087,"end":5110},{"type":"TextQuoteSelector","exact":"Curse of Dimensionality","prefix":"the state space is called this '","suffix":"'. It explains why state spaces "}]}]},{"text":"Q. Why can't we approximate the value function for a chess-playing RL agent with a lookup table? (give at least two reasons)\nA. e.g. the lookup table wouldn't fit in memory; too many states to visit during training","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[20]","startOffset":0,"endContainer":"/div[1]/p[20]","endOffset":108},{"type":"TextPositionSelector","start":6440,"end":6548},{"type":"TextQuoteSelector","exact":"When scaling these up to large state spaces there is a pair of thorny problems that we have to contend with.","prefix":"storing a value for every state.","suffix":"\nProblem 1: Memory SizeThis is p"}]}]},{"text":"Q. What's wrong with using low-degree polynomials to approximate value functions?\nA. Often too low-resolution to approximate the true value function.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/ol[2]/li[1]","startOffset":0,"endContainer":"/div[1]/ol[2]/li[1]/span[1]/span[3]","endOffset":0},{"type":"TextPositionSelector","start":13569,"end":13703},{"type":"TextQuoteSelector","exact":"Are too low-resolution with low-degree polynomials@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')∗^*∗","prefix":"hich is useful to be able to do)","suffix":"﻿Learn strange functions that do"}]}]},{"text":"Q. What's wrong with using high-degree polynomials to approximate value functions?\nA. They often overfit.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/ol[2]/li[1]/span[2]/span[3]","startOffset":0,"endContainer":"/div[1]/ol[3]/li[1]","endOffset":66},{"type":"TextPositionSelector","start":13703,"end":13770},{"type":"TextQuoteSelector","exact":"﻿Learn strange functions that don't generalise well at high degrees","prefix":"KaTeX/0.13.2/katex.min.css')∗^*∗","suffix":"\n@import url('https://cdnjs.clou"}]}]},{"text":"Q. What's meant by overfitting in machine learning?\nA. Finding a high-complexity solution which matches the training data, but performs poorly on inputs it hasn't seen.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[70]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[70]/strong[1]","endOffset":11},{"type":"TextPositionSelector","start":15589,"end":15600},{"type":"TextQuoteSelector","exact":"overfitting","prefix":"oint, it will. This is known as ","suffix":", which is a problem encountered"}]}]},{"text":"Q. What is is generalization in machine learning?\nA. How well a model makes predictions about areas of input space it hasn't seen.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[68]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[68]/strong[1]","endOffset":30},{"type":"TextPositionSelector","start":14989,"end":15019},{"type":"TextQuoteSelector","exact":"How well the model generalizes","prefix":"ise to this in the y-direction. ","suffix":" to the new datapoint (that it h"}]}]}]]