[[{"text":"Q. Define generalized policy iteration.\nA. The class of algorithms (including TD Learning) which alternates between policy evaluation and policy improvement.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[19]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[19]/strong[1]","endOffset":28},{"type":"TextPositionSelector","start":3439,"end":3467},{"type":"TextQuoteSelector","exact":"Generalised Policy Iteration","prefix":"greedily choose the next action.","suffix":" is the name of the broad class "}]}]},{"text":"Q. What does $v_*$ denote for a Markov decision process?\nA. The value function for the optimal policy, $\\pi_*$.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/figure[2]/figcaption[1]","startOffset":0,"endContainer":"/div[1]/figure[2]/figcaption[1]","endOffset":178},{"type":"TextPositionSelector","start":3719,"end":3897},{"type":"TextQuoteSelector","exact":"The star notation (@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')∗^*∗﻿) indicates the optimal policy and value function for that optimal policy.","prefix":"Policy Iteration, including TD.\n","suffix":" The funky arrows inside the loo"}]}]},{"text":"Q. In generalized policy iteration, what is policy evaluation?\nA. Finding the value function associated with the current policy.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/blockquote[1]/strong[1]","startOffset":0,"endContainer":"/div[1]/blockquote[1]","endOffset":113},{"type":"TextPositionSelector","start":4166,"end":4279},{"type":"TextQuoteSelector","exact":"Policy Evaluation is the process of updating a value function such that it is consistent with the current policy.","prefix":"function.\n\n\n2. Policy Evaluation","suffix":"We’ve done Policy Evaluation on "}]}]},{"text":"Q. Policies and value functions have a [one/many]-to-[one/many] relationship.\nA. Many-to-one. (Each policy has exactly one associated value function; a single value function may result from many policies.)","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/blockquote[2]","startOffset":0,"endContainer":"/div[1]/blockquote[2]/strong[1]","endOffset":47},{"type":"TextPositionSelector","start":4572,"end":4627},{"type":"TextQuoteSelector","exact":"Rule 1: Every policy has 1 corresponding value function","prefix":" uses the following simple rule:","suffix":"\nWe’ve seen examples of this in "}]}]},{"text":"Q. In generalized policy iteration, what is policy improvement?\nA. Improving the policy by acting greedily with respect to the current value function estimate.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[39]","startOffset":0,"endContainer":"/div[1]/p[39]","endOffset":110},{"type":"TextPositionSelector","start":5556,"end":5666},{"type":"TextQuoteSelector","exact":"Policy Improvement improves the policy followed by acting greedily with respect to the current value function.","prefix":"olicies.\n\n\n3. Policy Improvement","suffix":" This step is simple when we kno"}]}]},{"text":"Q. Define the policy improvement theorem.\nA. In the absence of function approximation, acting greedily with respect to a value function will not decrease the value of any state. (If $\\pi_2 = \\text{greedy} (v_{\\pi_1})$, then $v_{\\pi_2}(s) >= v_{\\pi_1}(s)$ for all $s$)","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[44]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[44]/strong[1]","endOffset":26},{"type":"TextPositionSelector","start":6442,"end":6468},{"type":"TextQuoteSelector","exact":"Policy Improvement Theorem","prefix":"of any state\nThis is called the ","suffix":".\nMathematically the policy impr"}]}]},{"text":"Q. In RL, what is a stochastic state transition function?\nA. Given a current state and an action to take, it defines a probability distribution over successor states.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[63]","startOffset":2,"endContainer":"/div[1]/p[63]","endOffset":38},{"type":"TextPositionSelector","start":9899,"end":9935},{"type":"TextQuoteSelector","exact":"stochastic state transition function","prefix":"g no randomness is involved).\nA ","suffix":" defines a probability distribut"}]}]},{"text":"Q. How do we find the action-value when our state transition function is stochastic?\nA. Take the expectation over the value of all possible successor states. (i.e. the sum weighted by the probability of each outcome)","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[65]","startOffset":155,"endContainer":"/div[1]/p[65]/strong[1]","endOffset":11},{"type":"TextPositionSelector","start":10626,"end":10649},{"type":"TextQuoteSelector","exact":"We take the expectation","prefix":"TeX/0.13.2/katex.min.css')aaa﻿? ","suffix":" (fancy word to mean we weight e"}]}]},{"text":"Q. During approximate generalized policy iteration, when can the policy get *worse* if the value function is a look-up table? \nA. If an update to the value function estimate makes it *less* accurate, which can happen when $\\alpha$ is too large.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[81]","startOffset":79,"endContainer":"/div[1]/p[81]","endOffset":130},{"type":"TextPositionSelector","start":12769,"end":12820},{"type":"TextQuoteSelector","exact":"unless an update to the value function goes too far","prefix":"e generalised policy iteration, ","suffix":", which only happens if the lear"}]}]},{"text":"Q. In what sense does TD Learning only approximate generalized policy iteration?\nA. We don't know the true value functions of our policies; policy improvement acts greedily with respect to *estimated* value functions.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[79]","startOffset":193,"endContainer":"/div[1]/p[79]","endOffset":233},{"type":"TextPositionSelector","start":12164,"end":12204},{"type":"TextQuoteSelector","exact":"approximate Generalised Policy Iteration","prefix":", which we update! Surely that ‘","suffix":"’ shouldn’t work?”.You’re right "}]}]},{"text":"Q. In generalized policy iteration, what conditions must hold when the policy and value function stabilize?\nA. The estimated value function matches the policy's true value function, and that policy is greedy with respect to its value function.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[83]","startOffset":0,"endContainer":"/div[1]/p[83]","endOffset":220},{"type":"TextPositionSelector","start":13301,"end":13521},{"type":"TextQuoteSelector","exact":"The policy and the value function only stabilise when the value function estimate is the value function of the policy and that policy is greedy with respect to its value function. This is only true at the optimal policy.","prefix":"erent actions in certain states.","suffix":"So Generalised Policy Iteration "}]}]},{"text":"Q. When using a lookup table to represent the value function, why is generalized policy iteration guaranteed to converge to the optimal policy?\nA. The Policy Improvement Theorem ensures every step of policy improvement will improve the policy.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[84]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[84]/strong[1]","endOffset":44},{"type":"TextPositionSelector","start":13556,"end":13600},{"type":"TextQuoteSelector","exact":"guaranteed to converge to the optimal policy","prefix":"Generalised Policy Iteration is ","suffix":", given enough time, computation"}]}]},{"text":"Q. Term for the broad class of RL algorithms which alternate between policy evaluation and policy improvement?\nA. Generalized policy iteration","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[19]","startOffset":32,"endContainer":"/div[1]/p[19]","endOffset":110},{"type":"TextPositionSelector","start":3454,"end":3532},{"type":"TextQuoteSelector","exact":"the name of the broad class of algorithms which switch between these two modes","prefix":"Generalised Policy Iteration is ","suffix":" - policy evaluation and policy "}]}]}]]